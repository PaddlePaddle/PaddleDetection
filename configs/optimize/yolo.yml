Optimize:
  learning_rate:
    base_lr: 0.01
    schedulers:
    - !PiecewiseDecay
      gamma: 0.1
      milestones: [200, 250]
    - !LinearWarmup
      start_factor: 0.
      steps: 4000
  optimizer:
    type: Momentum
    momentum: 0.9
  regularizer:
    factor: 0.0005
    type: L2
