epoch: 120 # employ iter to control shedule
LearningRate:
  base_lr: 0.02 # 0.02 for 64 batch
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones: [3000] # do not decay lr
  - !LinearWarmup
    start_factor: 0.3333333333333333
    steps: 1000

max_iter: 360000 # 360k for 32 batch, 720k for 16b tach
epoch_iter: 3000 # set epoch_iter for saving checkpoint and eval
optimize_rate: 1
SEMISUPNET:
  BBOX_THRESHOLD: 0.5 # # not used
  TEACHER_UPDATE_ITER: 1
  BURN_UP_STEP: -1
  EMA_KEEP_RATE: 0.9996
  UNSUP_LOSS_WEIGHT: 1.0
  # 20220817
  PSEUDO_WARM_UP_STEPS: 2000

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0001
    type: L2
