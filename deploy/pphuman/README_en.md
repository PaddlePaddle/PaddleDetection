English | [ç®€ä½“ä¸­æ–‡](README.md)

# Real-Time Pedestrian Analysis Tool â€”â€”â€”â€” PP-Human

PP-Human serves as the first open-source tool of real-time pedestrian anaylsis relying on the PaddlePaddle deep learning framework. It has three advantages: rich functions, wide application and efficient deployment.

![](https://user-images.githubusercontent.com/48054808/173030254-ecf282bd-2cfe-43d5-b598-8fed29e22020.gif)

PP-Human offers many input options, including image/single-camera video/multi-camera video, and covers multi-object tracking, attribute recognition, and action recognition. PP-Human can be applied to intelligent traffic, the intelligent community, industiral patrol, and so on. It supports server-side deployment and TensorRT accelerationï¼Œand achieves real-time analysis on the T4 server.

## ğŸ“£ Recent updates

- 2022.4.18ï¼šFull-process operation tutorial of PP-Human, covering training, deployment, action expansion, please refer to this [AI Studio project](https://aistudio.baidu.com/aistudio/projectdetail/3842982).

- 2022.4.10ï¼šCommunity intelligent management supportted by PP-Human, please refer to this [AI Studio project](https://aistudio.baidu.com/aistudio/projectdetail/3679564) for quick start tutorial.
- 2022.4.5ï¼šThe real-time pedestrian analysis tool PP-Human is released, which supports four capabilities: pedestrian tracking, pedestrian flow statistics, human attribute recognition and fall detection. It is specially optimized based on real scene data to accurately identify various fall postures and adapt to different environmental backgrounds, light and camera angles.


## ğŸ”® Function introduction and effect display

| â­ åŠŸèƒ½           | ğŸ’Ÿ æ–¹æ¡ˆä¼˜åŠ¿                                                                                                                                           | ğŸ’¡ç¤ºä¾‹å›¾                                                                                                                                         |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| **è·¨é•œè·Ÿè¸ªï¼ˆReIDï¼‰** | è¶…å¼ºæ€§èƒ½ï¼šé’ˆå¯¹ç›®æ ‡é®æŒ¡ã€å®Œæ•´åº¦ã€æ¨¡ç³Šåº¦ç­‰éš¾ç‚¹ç‰¹æ®Šä¼˜åŒ–ï¼Œå®ç°mAP 98.8ã€1.5ms/äºº                                                                                                     | <img src="https://user-images.githubusercontent.com/48054808/173037607-0a5deadc-076e-4dcc-bd96-d54eea205f1f.png" title="" alt="" width="191"> |
| **å±æ€§åˆ†æ**       | å…¼å®¹å¤šç§æ•°æ®æ ¼å¼ï¼šæ”¯æŒå›¾ç‰‡ã€è§†é¢‘è¾“å…¥<br/><br/>é«˜æ€§èƒ½ï¼šèåˆå¼€æºæ•°æ®é›†ä¸ä¼ä¸šçœŸå®æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå®ç°mAP 94.86ã€2ms/äºº<br/><br/>æ”¯æŒ26ç§å±æ€§ï¼šæ€§åˆ«ã€å¹´é¾„ã€çœ¼é•œã€ä¸Šè¡£ã€é‹å­ã€å¸½å­ã€èƒŒåŒ…ç­‰26ç§é«˜é¢‘å±æ€§                                | <img src="https://user-images.githubusercontent.com/48054808/173036043-68b90df7-e95e-4ada-96ae-20f52bc98d7c.png" title="" alt="" width="207"> |
| **è¡Œä¸ºè¯†åˆ«**       | åŠŸèƒ½ä¸°å¯Œï¼šæ”¯æŒæ‘”å€’ã€æ‰“æ¶ã€æŠ½çƒŸã€æ‰“ç”µè¯ã€äººå‘˜é—¯å…¥äº”ç§é«˜é¢‘å¼‚å¸¸è¡Œä¸ºè¯†åˆ«<br/><br/>é²æ£’æ€§å¼ºï¼šå¯¹å…‰ç…§ã€è§†è§’ã€èƒŒæ™¯ç¯å¢ƒæ— é™åˆ¶<br/><br/>æ€§èƒ½é«˜ï¼šä¸è§†é¢‘è¯†åˆ«æŠ€æœ¯ç›¸æ¯”ï¼Œæ¨¡å‹è®¡ç®—é‡å¤§å¹…é™ä½ï¼Œæ”¯æŒæœ¬åœ°åŒ–ä¸æœåŠ¡åŒ–å¿«é€Ÿéƒ¨ç½²<br/><br/>è®­ç»ƒé€Ÿåº¦å¿«ï¼šä»…éœ€15åˆ†é’Ÿå³å¯äº§å‡ºé«˜ç²¾åº¦è¡Œä¸ºè¯†åˆ«æ¨¡å‹ | <img src="https://user-images.githubusercontent.com/48054808/173034825-623e4f78-22a5-4f14-9b83-dc47aa868478.gif" title="" alt="" width="209"> |
| **äººæµé‡è®¡æ•°ä¸è½¨è¿¹è®°å½•** | ç®€æ´æ˜“ç”¨ï¼šå•ä¸ªå‚æ•°å³å¯å¼€å¯äººæµé‡è®¡æ•°ä¸è½¨è¿¹è®°å½•åŠŸèƒ½                                                                                                                         | <img src="https://user-images.githubusercontent.com/22989727/174736440-87cd5169-c939-48f8-90a1-0495a1fcb2b1.gif" title="" alt="" width="200"> |


## ğŸ—³ Model ZOO

| Task            | Scenario | Precision | Inference Speedï¼ˆFPSï¼‰ | Model Weights |Model Inference and Deployment |
| :---------:     |:---------:     |:---------------     | :-------:  | :------:      | :------:      |
| Object Detection(high-precision)        | Image/Video Input | mAP: 56.6  | 28.0ms           |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.pdparams) |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip) |
| Object Detection(light-weight)        | Image/Video Input | mAP: 53.2  | 22.1ms           |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.pdparams) |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip) |
| Object Tracking(high-precision)       | Image/Video Input | MOTA: 79.5  | 33.1ms           |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.pdparams) |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip) |
| Object Tracking(light-weight)       | Image/Video Input | MOTA: 69.1  | 27.2ms           |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.pdparams) |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip) |
| Attribute Recognition    | Image/Video Input  Attribute Recognition | mA: 94.86 |  2ms per person       | - |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/strongbaseline_r50_30e_pa100k.zip) |
| Keypoint Detection    | Video Input  Falling Recognition | AP: 87.1 | 2.9ms per person        | [Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.pdparams) |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip)
| Falling Recognition   |  Video Input  Falling Recognition  | Precision 96.43 |  2.7ms per person          | - |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/STGCN.zip) |
| ReID         | Multi-Target Multi-Camera Tracking   | mAP: 98.8 | 1.5ms per person    | - |[Link](https://bj.bcebos.com/v1/paddledet/models/pipeline/reid_model.zip) |

Then, unzip the downloaded model to the folder `./output_inference`.


## ğŸ“š Documentation tutorial
### [QUICK START](docs/tutorials/QUICK_STARTED.md)


### Pedestrian Attribute/Feature Recognition

* [QUICK START](docs/tutorials/attribute.md)

* [Development Tutorial](../../docs/advanced_tutorials/customization/attribute.md)
  * Data preparation
  * Model optimization
  * Add new attribute

### Action Recognition

* [QUICK START](docs/tutorials/action.md)
  * Fall detection

* [Development Tutorial](../../docs/advanced_tutorials/customization/action.md)
  * Scheme selection
  * Data preparation
  * Model optimization
  * Add new action


### Multi-Target Multi-Camera Tracking and ReID

* [QUICK START](docs/tutorials/mtmct.md)

* [Development Tutorial]()
  * Data preparation
  * Model optimization


### Passenger flow counting and track recording

* [QUICK START](docs/tutorials/mot.md)

* [Development Tutorial](../../docs/advanced_tutorials/customization/mot.md)
  * Data preparation
  * Model optimization
